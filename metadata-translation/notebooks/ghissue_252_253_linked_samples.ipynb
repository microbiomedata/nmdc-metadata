{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(os.path.expanduser(\"~/.nmdc_mongo.env\"))\n",
    "\n",
    "from nmdc_mongo import get_db\n",
    "\n",
    "db_share = get_db(\"dwinston_share\")\n",
    "db_scratch = get_db(\"dwinston_scratch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os.path\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "\n",
    "TOKEN_FILE = os.path.expanduser(\"~/token.nmdc-gcloud-api.pickle\")\n",
    "CREDENTIALS_FILE = os.path.expanduser('~/nmdc-gcloud-api-credentials.json')\n",
    "\n",
    "# If modifying these scopes, delete the token*.pickle file.\n",
    "SCOPES = [\n",
    "    'https://www.googleapis.com/auth/spreadsheets.readonly',\n",
    "    'https://www.googleapis.com/auth/drive.readonly'\n",
    "]\n",
    "\n",
    "def get_gcloud_api_creds(scopes=SCOPES, token_file=TOKEN_FILE, credentials_file=CREDENTIALS_FILE):\n",
    "    creds = None\n",
    "    # The file token.pickle stores the user's access and refresh tokens, and is\n",
    "    # created automatically when the authorization flow completes for the first\n",
    "    # time.\n",
    "    if os.path.exists(token_file):\n",
    "        with open(token_file, 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                credentials_file, scopes)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open(token_file, 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "    return creds\n",
    "\n",
    "def get_sheet_values(sheet_id, sheet_range):\n",
    "    creds = get_gcloud_api_creds()\n",
    "    sheets_service = build('sheets', 'v4', credentials=creds)\n",
    "\n",
    "    sheet = sheets_service.spreadsheets()\n",
    "    result = sheet.values().get(spreadsheetId=sheet_id,\n",
    "                                range=sheet_range).execute()\n",
    "    values = result.get('values', [])\n",
    "\n",
    "    if not values:\n",
    "        print('No data found.')\n",
    "        return []\n",
    "    else:\n",
    "        return [row for row in values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = get_sheet_values(sheet_id='1nZOJYiC2QN0hOn5nDj9y9mteWeGyzQQls17zH5mESww', sheet_range='Sheet1!A:E')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "\n",
    "\n",
    "def get_file(file_id):\n",
    "    creds = get_gcloud_api_creds()\n",
    "    drive_service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "    request = drive_service.files().get_media(fileId=file_id)\n",
    "    f = io.BytesIO()\n",
    "    downloader = MediaIoBaseDownload(f, request)\n",
    "    done = False\n",
    "    while done is False:\n",
    "        status, done = downloader.next_chunk()\n",
    "        print(\"Download %d%%.\" % int(status.progress() * 100))\n",
    "    \n",
    "    return f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download 100%.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "f = get_file('1XoSHcImd9LRlZb2nYNWucGtTzgqmdMd0')\n",
    "s = f.getvalue().decode(\"utf-8\")\n",
    "f.close()\n",
    "try:\n",
    "    stegen_sample_template = json.loads(s)\n",
    "except json.JSONDecodeError:\n",
    "    stegen_sample_template = json.loads(\n",
    "        s.replace('\\n', '')\\\n",
    "        .replace(\"$BIOSAMPLE_ID\", '\"$BIOSAMPLE_ID\"')\\\n",
    "        .replace(\"â€œ\", '\"')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "gold_pattern = re.compile(r\"Gb\\d+\")\n",
    "\n",
    "def prefix_sample_id(s):\n",
    "    if \":\" in s:\n",
    "        return s\n",
    "    elif re.fullmatch(gold_pattern, s):\n",
    "        return \"gold:\" + s\n",
    "    else:\n",
    "        return \"emsl:\" + s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "omics = []\n",
    "for i, row in enumerate(rows):\n",
    "    if i == 0: # skip header row\n",
    "        continue\n",
    "    omics.append({\n",
    "        \"omics_id\": row[0],\n",
    "        \"omics_type\": row[1],\n",
    "        \"sample_name\": row[2],\n",
    "        \"sample_id\": prefix_sample_id(row[3]),\n",
    "        \"new\": len(row) > 4 and row[4] == \"TRUE\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_ids = [\n",
    "    d[\"id\"] for d in\n",
    "    db_share.biosample_set.find({\"id\": {\"$in\": [o[\"sample_id\"] for o in omics]}}, [\"id\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert {o[\"sample_id\"] for o in omics if o[\"new\"]} == {o[\"sample_id\"] for o in omics} - set(existing_ids) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz import assoc_in, get_in\n",
    "\n",
    "def transform_in(doc, keys, fn):\n",
    "    initial = get_in(keys, doc)\n",
    "    transformed = fn(initial)\n",
    "    return assoc_in(doc, keys, transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_template(template, sample_id, sample_name):\n",
    "    doc = assoc_in(template, [\"id\"], sample_id)\n",
    "    doc = transform_in(\n",
    "        doc, [\"identifier\", \"has_raw_value\"],\n",
    "        lambda s: s.replace(\"$BIOSAMPLE_NAME\", sample_name)\n",
    "    )\n",
    "    doc = transform_in(\n",
    "        doc, [\"name\"],\n",
    "        lambda s: s.replace(\"$BIOSAMPLE_NAME\", sample_name)\n",
    "    )\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_subdocs_to_id_strings(doc):\n",
    "    keys_with_term_ids = [\n",
    "        k for k in doc\n",
    "        if isinstance(doc[k], dict)\n",
    "        and \"term\" in doc[k]\n",
    "        and \"id\" in doc[k][\"term\"]\n",
    "    ]\n",
    "    for k in keys_with_term_ids:\n",
    "        doc = assoc_in(doc, [k, \"term\"], doc[k][\"term\"][\"id\"])\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_samples = {}\n",
    "for o in omics:\n",
    "    if o[\"new\"]:\n",
    "        new_samples[o[\"sample_id\"]] = o[\"sample_name\"]\n",
    "\n",
    "docs = []\n",
    "\n",
    "for sample_id, sample_name in new_samples.items():\n",
    "    doc = fill_template(stegen_sample_template, sample_id, sample_name)\n",
    "    doc = term_subdocs_to_id_strings(doc)\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_doc_to_mongo_collection_validator(mongo_collection, doc):\n",
    "    doc_items = sorted([(k, v) for k, v in doc.items()])\n",
    "    validator_items = sorted([\n",
    "        (k, v) for k, v in\n",
    "        validator_for(mongo_collection)['properties'].items()\n",
    "        if k in list(doc)\n",
    "    ])\n",
    "    idx_vi = 0\n",
    "    for k, v in doc_items:\n",
    "        print(\"\\n#####\"+k+\"\\n\")\n",
    "        print(\"doc value:\")\n",
    "        pprint(k)\n",
    "        print()\n",
    "        print(\"validator spec:\")\n",
    "        for vi in validator_items:\n",
    "            if vi[0] == k:\n",
    "                pprint(vi[1])\n",
    "                break\n",
    "        else:\n",
    "            print(f\"NO SPEC FOR {k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to update mongo collection schema to account for updated fields in nmdc.schema.json:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating biosample_set\n"
     ]
    }
   ],
   "source": [
    "from nmdc_mongo import dbschema, validator_for, collschemas_for\n",
    "from nmdc_mongo.admin import admin_client, reset_database_schema\n",
    "\n",
    "dbschema = assoc_in(dbschema, [\"definitions\", \"ControlledTermValue\", \"properties\", \"term\", \"type\"], \"string\")\n",
    "del dbschema[\"definitions\"][\"ControlledTermValue\"][\"properties\"][\"term\"][\"$ref\"]\n",
    "collschemas = collschemas_for(dbschema)\n",
    "\n",
    "targetdb_as_admin = admin_client[\"dwinston_share\"]\n",
    "target_collection_names = [\"biosample_set\"]\n",
    "reset_database_schema(targetdb_as_admin, target_collection_names, collschemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonschema\n",
    "\n",
    "jsonschema.validate({\"biosample_set\": docs}, schema=dbschema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import ReplaceOne\n",
    "\n",
    "rv = db_share.biosample_set.bulk_write([ReplaceOne({\"id\": doc[\"id\"]}, doc, upsert=True) for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rv.upserted_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second checklist item of GH Issue 252"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "omics = [\n",
    "    transform_in(o, [\"omics_id\"], lambda s: \"emsl:\"+s if \":\" not in s else s)\n",
    "    for o in omics\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "omics_ids = [o[\"omics_id\"] for o in omics]\n",
    "\n",
    "found_omics_ids = [\n",
    "    d[\"id\"] for d in\n",
    "    db_share.omics_processing_set.find({\"id\": {\"$in\": omics_ids}},[\"id\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(omics_ids) == set(found_omics_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "omics_updates = {}\n",
    "for o in omics:\n",
    "    omics_updates[o[\"omics_id\"]] = o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz import dissoc\n",
    "\n",
    "replacing_omics_type = {}\n",
    "\n",
    "requests = []\n",
    "\n",
    "for doc in db_share.omics_processing_set.find({\"id\": {\"$in\": omics_ids}}):\n",
    "    omics_type = get_in([\"omics_type\", \"has_raw_value\"], doc)\n",
    "    updates = omics_updates[doc[\"id\"]]\n",
    "    if omics_type != updates[\"omics_type\"]:\n",
    "        replacing_omics_type[doc[\"id\"]] = {\"from\": omics_type, \"to\": updates[\"omics_type\"]}\n",
    "    doc = assoc_in(doc, [\"omics_type\", \"has_raw_value\"], updates[\"omics_type\"])\n",
    "    doc = assoc_in(doc, [\"has_input\"], [updates[\"sample_id\"]])\n",
    "    requests.append(ReplaceOne({\"id\": doc[\"id\"]}, dissoc(doc, \"_id\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacing_omics_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "434"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv = db_share.omics_processing_set.bulk_write(requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "434"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rv.modified_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmdc",
   "language": "python",
   "name": "nmdc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
