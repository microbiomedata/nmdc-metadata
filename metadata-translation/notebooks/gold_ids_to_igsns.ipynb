{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent\n",
    "import os\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "\n",
    "import jsonschema\n",
    "import requests\n",
    "from mongospawn.schema import dbschema_from_file, collschemas_for\n",
    "from pymongo import MongoClient, ReplaceOne\n",
    "from toolz import keyfilter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "nmdc_schema_json_path = str(\n",
    "    Path.cwd().parent.parent.joinpath(\"schema\", \"nmdc.schema.json\")\n",
    ")\n",
    "dbschema = dbschema_from_file(nmdc_schema_json_path)\n",
    "collschemas = collschemas_for(dbschema)\n",
    "\n",
    "\n",
    "def reset_database(db):\n",
    "    for coll_name in collschemas:\n",
    "        db.drop_collection(coll_name)\n",
    "        db.create_collection(\n",
    "            coll_name, validator={\"$jsonSchema\": collschemas[coll_name]}\n",
    "        )\n",
    "        db[coll_name].create_index(\"id\", unique=True)\n",
    "\n",
    "\n",
    "def jsonschema_for(collection_name=None):\n",
    "    if collection_name not in set(dbschema[\"properties\"]):\n",
    "        raise ValueError(\n",
    "            f'collection_name must be one of {set(dbschema[\"properties\"])}'\n",
    "        )\n",
    "    defn = dbschema[\"properties\"][collection_name][\"items\"][\"$ref\"].split(\"/\")[-1]\n",
    "    return dbschema[\"definitions\"][defn]\n",
    "\n",
    "\n",
    "def validator_for(collection):\n",
    "    return collection.options()[\"validator\"][\"$jsonSchema\"]\n",
    "\n",
    "\n",
    "def pick(whitelist, d):\n",
    "    return keyfilter(lambda k: k in whitelist, d)\n",
    "\n",
    "\n",
    "def conform(doc, collection_name=None):\n",
    "    \"\"\"Provides limited, conservative conformance on a docments.\n",
    "\n",
    "    - If additionalProperties is False, omit any supplied.\n",
    "    - If a field must be a list of strings, and a lone string is supplied, wrap it in a list.\n",
    "\n",
    "    \"\"\"\n",
    "    if collection_name not in set(dbschema[\"properties\"]):\n",
    "        raise ValueError(\n",
    "            f'collection_name must be one of {set(dbschema[\"properties\"])}'\n",
    "        )\n",
    "    defn = dbschema[\"properties\"][collection_name][\"items\"][\"$ref\"].split(\"/\")[-1]\n",
    "    schema = dbschema[\"definitions\"][defn]\n",
    "    if schema.get(\"additionalProperties\") is False:\n",
    "        doc = pick(list(schema[\"properties\"]), doc)\n",
    "    for k in list(doc.keys()):\n",
    "        if (\n",
    "            isinstance(doc[k], str)\n",
    "            and schema[\"properties\"].get(k, {}).get(\"type\") == \"array\"\n",
    "            and schema[\"properties\"][k][\"items\"][\"type\"] == \"string\"\n",
    "            and not isinstance(doc[k], list)\n",
    "        ):\n",
    "            doc[k] = [doc[k]]\n",
    "    return doc\n",
    "\n",
    "\n",
    "def validate(doc, collection_name=None, conform_doc=False):\n",
    "    if collection_name not in set(dbschema[\"properties\"]):\n",
    "        raise ValueError(\n",
    "            f'collection_name must be one of {set(dbschema[\"properties\"])}'\n",
    "        )\n",
    "    if conform_doc:\n",
    "        doc = conform(doc, collection_name=collection_name)\n",
    "    jsonschema.validate(doc, schema=dbschema)\n",
    "    return doc\n",
    "\n",
    "\n",
    "def fetch_json(url):\n",
    "    return requests.get(url).json()\n",
    "\n",
    "\n",
    "def fetch_and_validate_json(resource, collection_name=None, conform_doc=False):\n",
    "    \"\"\"Takes a URL or the pre-fetched resource (list or dict)\"\"\"\n",
    "    payload = fetch_json(resource) if isinstance(resource, str) else resource\n",
    "    validated = []\n",
    "    if isinstance(payload, list):\n",
    "        for doc in tqdm(payload):\n",
    "            validated.append(\n",
    "                validate(doc, collection_name=collection_name, conform_doc=conform_doc)\n",
    "            )\n",
    "    elif isinstance(payload, dict):\n",
    "        if set(payload) & set(dbschema[\"properties\"]):\n",
    "            for collection_name, docs in payload.items():\n",
    "                for doc in tqdm(docs, desc=collection_name):\n",
    "                    validated.append(\n",
    "                        validate(\n",
    "                            doc,\n",
    "                            collection_name=collection_name,\n",
    "                            conform_doc=conform_doc,\n",
    "                        )\n",
    "                    )\n",
    "        else:\n",
    "            validated.append(\n",
    "                validate(\n",
    "                    payload, collection_name=collection_name, conform_doc=conform_doc\n",
    "                )\n",
    "            )\n",
    "    else:\n",
    "        raise ValueError(f\"Fetched JSON must be a JSON array or object\")\n",
    "    return validated\n",
    "\n",
    "\n",
    "def add_to_db(validated, db, collection_name=None):\n",
    "    if collection_name not in set(dbschema[\"properties\"]):\n",
    "        raise ValueError(\n",
    "            f'collection_name must be one of {set(dbschema[\"properties\"])}'\n",
    "        )\n",
    "    if isinstance(validated, list):\n",
    "        db[collection_name].bulk_write(\n",
    "            [ReplaceOne({\"id\": v[\"id\"]}, v, upsert=True) for v in validated]\n",
    "        )\n",
    "    elif isinstance(validated, dict):\n",
    "        if set(validated) & set(dbschema[\"properties\"]):\n",
    "            for collection_name, docs in validated.items():\n",
    "                db[collection_name].bulk_write(\n",
    "                    [ReplaceOne({\"id\": v[\"id\"]}, v, upsert=True) for v in docs]\n",
    "                )\n",
    "        else:\n",
    "            db[collection_name].bulk_write(\n",
    "                [ReplaceOne({\"id\": validated[\"id\"]}, validated, upsert=True)]\n",
    "            )\n",
    "    else:\n",
    "        raise ValueError(f\"payload must be a list or dict\")\n",
    "\n",
    "\n",
    "def fetch_conform_and_persist(spec, db):\n",
    "    url = spec[\"url\"]\n",
    "    collection_name = spec[\"type\"]\n",
    "    print(f\"fetching {url} ({collection_name})\")\n",
    "    payload = fetch_and_validate_json(url, collection_name, conform_doc=True)\n",
    "    add_to_db(payload, db, collection_name)\n",
    "\n",
    "\n",
    "def fetch_conform_and_persist_from_manifest(spec, db):\n",
    "    error_urls = []\n",
    "    url_manifest = spec[\"url_manifest\"]\n",
    "    collection_name = spec[\"type\"]\n",
    "    urls = fetch_json(url_manifest)\n",
    "\n",
    "    pbar = tqdm(total=len(urls))\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future_to_url = {\n",
    "            executor.submit(\n",
    "                fetch_and_validate_json, url, collection_name, conform_doc=True\n",
    "            ): url\n",
    "            for url in urls\n",
    "        }\n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            pbar.update(1)\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                payload = future.result()\n",
    "            except Exception as e:\n",
    "                error_urls.append((url, str(e)))\n",
    "            else:\n",
    "                add_to_db(payload, db, collection_name)\n",
    "\n",
    "    pbar.close()\n",
    "    return error_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from functools import partial, reduce\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pprint import pprint\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from dictdiffer import diff\n",
    "from pymongo import DeleteMany, DeleteOne, InsertOne, MongoClient, ReplaceOne, UpdateOne\n",
    "from toolz import assoc_in, compose, concat, dissoc, keyfilter, get_in, merge, merge_with\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Re-)load existing NMDC DB from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile('../src/data/nmdc_database.json.zip') as myzip:\n",
    "    with myzip.open('nmdc_database.json') as f:\n",
    "        nmdc_database = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['activity_set', 'biosample_set', 'data_object_set', 'functional_annotation_set', 'genome_feature_set', 'mags_activity_set', 'omics_processing_set', 'study_set']\n"
     ]
    }
   ],
   "source": [
    "client = MongoClient(\n",
    "    host=os.getenv(\"NMDC_MONGO_HOST\"),\n",
    "    username=\"dwinston_rw\",\n",
    "    password=os.getenv(\"NMDC_MONGO_RW_PWD\"))\n",
    "dbname = \"dwinston_scratch\"\n",
    "db = client[dbname]\n",
    "\n",
    "for collection in nmdc_database:\n",
    "    db.drop_collection(collection)\n",
    "    db[collection].create_index(\"id\", unique=True)\n",
    "    db[collection].insert_many(nmdc_database[collection])\n",
    "print(sorted(db.list_collection_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load FICUS Brodie spreadsheet and create gold-id-to-igsn map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOLD_ID_IDX = 5\n",
    "IGSN_IDX = 2\n",
    "\n",
    "igsn_golds = defaultdict(list)\n",
    "\n",
    "gold_id_pattern = re.compile(r\"Gb\\d+\")\n",
    "\n",
    "with open('../src/data/FICUS_Soil_Gs0135149_Brodie-12-23-2020_PS.xlsx - Brodie_Gs0135149_Soil_Metadata.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        gold_id = row[GOLD_ID_IDX]\n",
    "        igsn = row[IGSN_IDX]\n",
    "        if gold_id_pattern.fullmatch(gold_id):\n",
    "            igsn_golds[igsn].append(gold_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare helper function to compare timestamps given in e.g. \"15-MAY-20 08.30.01.000000000 am\" format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_pattern = re.compile(r\"\\d{2}-(?P<month>\\w+)-\\d{2} \\d{2}\\.\\d{2}\\.\\d{2}\\.(?P<ns>\\d+) [A|P]M\")\n",
    "dt_format = \"%d-%b-%y %I.%M.%S.%f %p\"\n",
    "\n",
    "def order_timestamps(timestamps):\n",
    "    if not all(isinstance(ts, str) for ts in timestamps):\n",
    "        raise Exception(f\"{timestamps} not strings\")\n",
    "    as_datetimes = []\n",
    "    for ts in timestamps:\n",
    "        match = dt_pattern.search(ts)\n",
    "        first, month, rest = ts.partition(match.group(\"month\"))\n",
    "        ts_new = first + month[0] + month[1:].lower() + rest\n",
    "        ts_new = ts_new.replace(match.group(\"ns\"), match.group(\"ns\")[:-3]) # truncate to microseconds\n",
    "        as_datetimes.append(datetime.strptime(ts_new, dt_format))\n",
    "    sorted_dts = sorted(as_datetimes)\n",
    "    return [dt.strftime(dt_format) for dt in sorted_dts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare helper-function pipeline to unify biosample_set documents that should be considered equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "er_xna_pattern = re.compile(r\"ER_[D|R]NA_\\d+$\")\n",
    "\n",
    "def rstrip_name_ER_ID(d):\n",
    "    s = get_in([\"name\"], d)\n",
    "    s_new = er_xna_pattern.split(s)[0] if er_xna_pattern.search(s) else s\n",
    "    return assoc_in(d, [\"name\"], s_new)\n",
    "\n",
    "def capitalize_location_raw_value(d):\n",
    "    s = get_in([\"location\", \"has_raw_value\"], d)\n",
    "    s_new = s[0].upper() + s[1:]\n",
    "    return assoc_in(d, [\"location\", \"has_raw_value\"], s_new)\n",
    "\n",
    "pipeline = compose(\n",
    "    capitalize_location_raw_value,\n",
    "    rstrip_name_ER_ID,\n",
    "    lambda d: dissoc(d, \"_id\", \"id\", \"add_date\", \"mod_date\", \"identifier\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce new biosample objects with ISGN ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_biosample_docs = []\n",
    "\n",
    "for igsn, golds in igsn_golds.items():\n",
    "    igsn_curie = \"igsn:\"+igsn\n",
    "    to_change = list(db.biosample_set.find({\"id\": {\"$in\": [f\"gold:{g}\" for g in golds]}}))\n",
    "    \n",
    "    # No merge needed, just change of id.\n",
    "    if len(to_change) == 1:\n",
    "        merged = assoc_in(to_change[0], [\"id\"], igsn_curie)\n",
    "        merged = assoc_in(merged, [\"identifier\", \"has_raw_value\"], igsn_curie)\n",
    "        merged_biosample_docs.append(merged)\n",
    "        continue\n",
    "    elif len(to_change) == 0:\n",
    "        continue\n",
    "\n",
    "    # Ensure that unification pipeline is adequate to resolve differences.\n",
    "    distilled = list(map(pipeline, to_change))\n",
    "    result = list(diff(distilled[0], distilled[1]))\n",
    "    assert result == []\n",
    "    \n",
    "    # Produce a merged document\n",
    "    earlier_ts, _ = order_timestamps([get_in([\"add_date\", \"has_raw_value\"], d) for d in to_change])\n",
    "    merged = assoc_in(distilled[0], [\"add_date\", \"has_raw_value\"], earlier_ts)\n",
    "    _, later_ts = order_timestamps([get_in([\"mod_date\", \"has_raw_value\"], d) for d in to_change])\n",
    "    merged = assoc_in(merged, [\"mod_date\", \"has_raw_value\"], later_ts)\n",
    "    merged = assoc_in(merged, [\"id\"], igsn_curie)\n",
    "    merged = assoc_in(merged, [\"identifier\", \"has_raw_value\"], igsn_curie)\n",
    "    \n",
    "    merged_biosample_docs.append(merged)\n",
    "    merged = None # defense against accidental reuse during next iteration.\n",
    "\n",
    "assert len(merged_biosample_docs) == len(igsn_golds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete old biosample objects and insert new ones in one bulk-write operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93, 48)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests = [DeleteMany({\"id\": {\"$in\": [\"gold:\"+g for g in concat(igsn_golds.values())]}})]\n",
    "requests.extend([InsertOne(d) for d in merged_biosample_docs])\n",
    "result = db.biosample_set.bulk_write(requests)\n",
    "result.deleted_count, result.inserted_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update omics_processing_set references to biosample_set ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldid_igsn = {}\n",
    "for igsn, gids in igsn_golds.items():\n",
    "    for gid in gids:\n",
    "        goldid_igsn[gid] = igsn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests = []\n",
    "to_replace = {\"gold:\"+k: \"igsn:\"+v for k, v in goldid_igsn.items()}\n",
    "\n",
    "for doc in db.omics_processing_set.find({\"$or\": [\n",
    "    {\"id\": {\"$in\": list(to_replace)}},\n",
    "    {\"has_input\": {\"$in\": list(to_replace)}},\n",
    "    {\"has_output\": {\"$in\": list(to_replace)}},\n",
    "    {\"part_of\": {\"$in\": list(to_replace)}}\n",
    "]}):\n",
    "    operations = {\"$set\": {\n",
    "        \"id\": to_replace.get(doc[\"id\"], doc[\"id\"]),\n",
    "        \"has_input\": [to_replace.get(i, i) for i in doc[\"has_input\"]],\n",
    "        \"has_output\": [to_replace.get(i, i) for i in doc[\"has_output\"]],\n",
    "        \"part_of\": [to_replace.get(i, i) for i in doc[\"part_of\"]],\n",
    "    }}\n",
    "    requests.append({\"filter\": {\"_id\": doc[\"_id\"]}, \"update\": operations})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv = db.omics_processing_set.bulk_write([UpdateOne(**r) for r in requests])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rv.modified_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update omics_processing_set references from EMSL ids to IGSNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMSL_IDS_IDX = 7\n",
    "IGSN_IDX = 2\n",
    "\n",
    "igsn_emsls = {}\n",
    "\n",
    "emsl_ids_pattern = re.compile(r\"\\d+\")\n",
    "\n",
    "with open('../src/data/FICUS_Soil_Gs0135149_Brodie-12-23-2020_PS.xlsx - Brodie_Gs0135149_Soil_Metadata.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        emsl_ids = row[EMSL_IDS_IDX]\n",
    "        igsn = row[IGSN_IDX]\n",
    "        ids = emsl_ids_pattern.findall(emsl_ids)\n",
    "        # XXX some rows have emsl ids but no IGSN, so igsn.strip() check here\n",
    "        if igsn.strip() and ids:\n",
    "            igsn_emsls[igsn] = ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "emslid_igsn = {}\n",
    "for igsn, eids in igsn_emsls.items():\n",
    "    for eid in eids:\n",
    "        emslid_igsn[eid] = igsn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_with_emsl_id = db.omics_processing_set.count_documents(\n",
    "    {\"id\": {\"$in\": [\"emsl:\"+i for i in emslid_igsn]}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests = []\n",
    "to_replace = {\"emsl:\"+k: \"igsn:\"+v for k, v in emslid_igsn.items()}\n",
    "to_replace.update({\"emsl:output_\"+k: \"igsn:\"+v for k, v in emslid_igsn.items()})\n",
    "\n",
    "def omit(blacklist, d):\n",
    "    return keyfilter(lambda k: k not in blacklist, d)\n",
    "\n",
    "def sans_mongo_id(d):\n",
    "    return omit([\"_id\"], d)\n",
    "\n",
    "for doc in db.omics_processing_set.find({\"$or\": [\n",
    "    {\"id\": {\"$in\": list(to_replace)}},\n",
    "    {\"has_input\": {\"$in\": list(to_replace)}},\n",
    "    {\"has_output\": {\"$in\": list(to_replace)}},\n",
    "    {\"part_of\": {\"$in\": list(to_replace)}}\n",
    "]}):\n",
    "    new_id = to_replace.get(doc[\"id\"], doc[\"id\"])\n",
    "    new_doc = sans_mongo_id(doc)\n",
    "    operations = {}\n",
    "    for field in [\"has_input\", \"has_output\", \"part_of\"]:\n",
    "        if field in doc:\n",
    "            operations = assoc_in(operations, [\"$set\", field], [to_replace.get(i, i) for i in doc[field]])\n",
    "    if operations:\n",
    "        new_doc = merge(new_doc, operations[\"$set\"])\n",
    "    if new_id != doc[\"id\"]:\n",
    "        new_doc = assoc_in(new_doc, [\"id\"], new_id)\n",
    "        requests.append({\"type\": \"deleteone\", \"filter\": {\"id\": doc[\"id\"]}})\n",
    "    requests.append({\"type\": \"replaceone\", \"filter\": {\"id\": new_id}, \"replacement\": new_doc, \"upsert\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pymongo_requests = []\n",
    "for r in requests:\n",
    "    t = r[\"type\"]\n",
    "    rest = omit([\"type\"], r)\n",
    "    if t == \"deleteone\":\n",
    "        pymongo_requests.append(DeleteOne(**rest))\n",
    "    elif t == \"replaceone\":\n",
    "        pymongo_requests.append(ReplaceOne(**rest))\n",
    "        \n",
    "rv = db.omics_processing_set.bulk_write(pymongo_requests, ordered=True)\n",
    "assert n_with_emsl_id == rv.modified_count + rv.upserted_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some newly IGSN id'ed docs identify as part_of biosamples without IGSNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('600f1b0425261d62ada0d593'),\n",
       " 'id': 'igsn:IEWFS0001',\n",
       " 'name': 'EMSL_49991_Brodie_115_Lipids_POS_09Aug19_Lola-WCSH417820',\n",
       " 'description': 'High res MS with high res HCD MSn and low res CID MSn',\n",
       " 'part_of': ['gold:Gs0135149'],\n",
       " 'has_output': ['igsn:IEWFS0001'],\n",
       " 'omics_type': {'has_raw_value': 'Lipidomics'},\n",
       " 'type': 'nmdc:OmicsProcessing',\n",
       " 'instrument_name': {'has_raw_value': 'VOrbiETD04'},\n",
       " 'processing_institution': {'has_raw_value': 'Environmental Molecular Sciences Lab'}}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.omics_processing_set.find_one(\n",
    "    {\"id\": {\"$regex\": \"^igsn\"}, \"part_of\": {\"$regex\": \"^gold\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"gold:Gs0135149\" in goldid_igsn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b7877dd2874808a17aed1d83521e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "all collections:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20184da86d3945a3a7ae409bc9790e59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validating genome_feature_set: |          | 0/0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa3a9075f7940a78d9c9949789e5169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adding genome_feature_set: |          | 0/0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb337da26454b7aa2a3024af99870b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validating omics_processing_set:   0%|          | 0/6764 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed702150832f4478a3f626bdfb2d8e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adding omics_processing_set:   0%|          | 0/6764 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc450af3749840349cdf0994eb49e589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validating mags_activity_set:   0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d8fa5f3426c4822b745dcc1f523483e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adding mags_activity_set:   0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d39f1957da400c9c8b092ab00d5dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validating functional_annotation_set: |          | 0/0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ae4d78520c4e20b97fd661314a177a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adding functional_annotation_set: |          | 0/0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0463bf6d505a453aa4b9e5dcf5b54679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validating activity_set:   0%|          | 0/1033 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae1a6c879a3347a398630b0485580e4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adding activity_set:   0%|          | 0/1033 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f631f141464257b2e5f19dc242e47d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validating data_object_set:   0%|          | 0/10870 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0c4bbdc9a3410490a014cbfd079ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adding data_object_set:   0%|          | 0/10870 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac87ae0f1ef4a0695f51aa1327abf92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validating study_set:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06abf289a3540e9a67d60392a5f95e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adding study_set:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c0ee860a53461ebfee515d26034005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validating biosample_set:   0%|          | 0/919 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab503656204344809dca14424fe2a159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adding biosample_set:   0%|          | 0/919 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "db_share = client[\"dwinston_dev\"]\n",
    "\n",
    "for collection_name in tqdm(db.list_collection_names(), desc=\"all collections\"):\n",
    "    docs = [sans_mongo_id(doc) for doc in db[collection_name].find()]\n",
    "    for doc in docs:\n",
    "        if \"lat_lon\" in doc and \"type\" in doc[\"lat_lon\"]:\n",
    "            del doc[\"lat_lon\"][\"type\"]\n",
    "        for k in doc:\n",
    "            if isinstance(doc[k], dict) and \"term\" in doc[k] and \"id\" in doc[k][\"term\"]:\n",
    "                doc[k][\"term\"] = doc[k][\"term\"][\"id\"]\n",
    "    validated = []\n",
    "    for doc in tqdm(docs, desc=\"validating \"+collection_name):\n",
    "        validated.append(validate(doc, collection_name=collection_name, conform_doc=True))\n",
    "    for doc in tqdm(validated, desc=\"adding \"+collection_name):\n",
    "        add_to_db(doc, db_share, collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmdc",
   "language": "python",
   "name": "nmdc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
