{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent\n",
    "import os\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "\n",
    "import jsonschema\n",
    "import requests\n",
    "from mongospawn.schema import dbschema_from_file, collschemas_for\n",
    "from pymongo import MongoClient, ReplaceOne\n",
    "from toolz import keyfilter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "nmdc_schema_json_path = str(\n",
    "    Path.cwd().parent.parent.joinpath(\"schema\", \"nmdc.schema.json\")\n",
    ")\n",
    "dbschema = dbschema_from_file(nmdc_schema_json_path)\n",
    "collschemas = collschemas_for(dbschema)\n",
    "\n",
    "\n",
    "def reset_database(db):\n",
    "    for coll_name in collschemas:\n",
    "        db.drop_collection(coll_name)\n",
    "        db.create_collection(\n",
    "            coll_name, validator={\"$jsonSchema\": collschemas[coll_name]}\n",
    "        )\n",
    "        db[coll_name].create_index(\"id\", unique=True)\n",
    "\n",
    "\n",
    "def jsonschema_for(collection_name=None):\n",
    "    if collection_name not in set(dbschema[\"properties\"]):\n",
    "        raise ValueError(\n",
    "            f'collection_name must be one of {set(dbschema[\"properties\"])}'\n",
    "        )\n",
    "    defn = dbschema[\"properties\"][collection_name][\"items\"][\"$ref\"].split(\"/\")[-1]\n",
    "    return dbschema[\"definitions\"][defn]\n",
    "\n",
    "\n",
    "def validator_for(collection):\n",
    "    return collection.options()[\"validator\"][\"$jsonSchema\"]\n",
    "\n",
    "\n",
    "def pick(whitelist, d):\n",
    "    return keyfilter(lambda k: k in whitelist, d)\n",
    "\n",
    "\n",
    "def conform(doc, collection_name=None):\n",
    "    \"\"\"Provides limited, conservative conformance on a docments.\n",
    "\n",
    "    - If additionalProperties is False, omit any supplied.\n",
    "    - If a field must be a list of strings, and a lone string is supplied, wrap it in a list.\n",
    "\n",
    "    \"\"\"\n",
    "    if collection_name not in set(dbschema[\"properties\"]):\n",
    "        raise ValueError(\n",
    "            f'collection_name must be one of {set(dbschema[\"properties\"])}'\n",
    "        )\n",
    "    defn = dbschema[\"properties\"][collection_name][\"items\"][\"$ref\"].split(\"/\")[-1]\n",
    "    schema = dbschema[\"definitions\"][defn]\n",
    "    if schema.get(\"additionalProperties\") is False:\n",
    "        doc = pick(list(schema[\"properties\"]), doc)\n",
    "    for k in list(doc.keys()):\n",
    "        if (\n",
    "            isinstance(doc[k], str)\n",
    "            and schema[\"properties\"].get(k, {}).get(\"type\") == \"array\"\n",
    "            and schema[\"properties\"][k][\"items\"][\"type\"] == \"string\"\n",
    "            and not isinstance(doc[k], list)\n",
    "        ):\n",
    "            doc[k] = [doc[k]]\n",
    "    return doc\n",
    "\n",
    "\n",
    "def validate(doc, collection_name=None, conform_doc=False):\n",
    "    if collection_name not in set(dbschema[\"properties\"]):\n",
    "        raise ValueError(\n",
    "            f'collection_name must be one of {set(dbschema[\"properties\"])}'\n",
    "        )\n",
    "    if conform_doc:\n",
    "        doc = conform(doc, collection_name=collection_name)\n",
    "    jsonschema.validate(doc, schema=dbschema)\n",
    "    return doc\n",
    "\n",
    "\n",
    "def fetch_json(url):\n",
    "    return requests.get(url).json()\n",
    "\n",
    "\n",
    "def fetch_and_validate_json(resource, collection_name=None, conform_doc=False):\n",
    "    \"\"\"Takes a URL or the pre-fetched resource (list or dict)\"\"\"\n",
    "    payload = fetch_json(resource) if isinstance(resource, str) else resource\n",
    "    validated = []\n",
    "    if isinstance(payload, list):\n",
    "        for doc in tqdm(payload):\n",
    "            validated.append(\n",
    "                validate(doc, collection_name=collection_name, conform_doc=conform_doc)\n",
    "            )\n",
    "    elif isinstance(payload, dict):\n",
    "        if set(payload) & set(dbschema[\"properties\"]):\n",
    "            for collection_name, docs in payload.items():\n",
    "                for doc in tqdm(docs, desc=collection_name):\n",
    "                    validated.append(\n",
    "                        validate(\n",
    "                            doc,\n",
    "                            collection_name=collection_name,\n",
    "                            conform_doc=conform_doc,\n",
    "                        )\n",
    "                    )\n",
    "        else:\n",
    "            validated.append(\n",
    "                validate(\n",
    "                    payload, collection_name=collection_name, conform_doc=conform_doc\n",
    "                )\n",
    "            )\n",
    "    else:\n",
    "        raise ValueError(f\"Fetched JSON must be a JSON array or object\")\n",
    "    return validated\n",
    "\n",
    "\n",
    "def add_to_db(validated, db, collection_name=None):\n",
    "    if collection_name not in set(dbschema[\"properties\"]):\n",
    "        raise ValueError(\n",
    "            f'collection_name must be one of {set(dbschema[\"properties\"])}'\n",
    "        )\n",
    "    if isinstance(validated, list):\n",
    "        db[collection_name].bulk_write(\n",
    "            [ReplaceOne({\"id\": v[\"id\"]}, v, upsert=True) for v in validated]\n",
    "        )\n",
    "    elif isinstance(validated, dict):\n",
    "        if set(validated) & set(dbschema[\"properties\"]):\n",
    "            for collection_name, docs in validated.items():\n",
    "                db[collection_name].bulk_write(\n",
    "                    [ReplaceOne({\"id\": v[\"id\"]}, v, upsert=True) for v in docs]\n",
    "                )\n",
    "        else:\n",
    "            db[collection_name].bulk_write(\n",
    "                [ReplaceOne({\"id\": validated[\"id\"]}, validated, upsert=True)]\n",
    "            )\n",
    "    else:\n",
    "        raise ValueError(f\"payload must be a list or dict\")\n",
    "\n",
    "\n",
    "def fetch_conform_and_persist(spec, db):\n",
    "    url = spec[\"url\"]\n",
    "    collection_name = spec[\"type\"]\n",
    "    print(f\"fetching {url} ({collection_name})\")\n",
    "    payload = fetch_and_validate_json(url, collection_name, conform_doc=True)\n",
    "    add_to_db(payload, db, collection_name)\n",
    "\n",
    "\n",
    "def fetch_conform_and_persist_from_manifest(spec, db):\n",
    "    error_urls = []\n",
    "    url_manifest = spec[\"url_manifest\"]\n",
    "    collection_name = spec[\"type\"]\n",
    "    urls = fetch_json(url_manifest)\n",
    "\n",
    "    pbar = tqdm(total=len(urls))\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future_to_url = {\n",
    "            executor.submit(\n",
    "                fetch_and_validate_json, url, collection_name, conform_doc=True\n",
    "            ): url\n",
    "            for url in urls\n",
    "        }\n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            pbar.update(1)\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                payload = future.result()\n",
    "            except Exception as e:\n",
    "                error_urls.append((url, str(e)))\n",
    "            else:\n",
    "                add_to_db(payload, db, collection_name)\n",
    "\n",
    "    pbar.close()\n",
    "    return error_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(\n",
    "    host=os.getenv(\"NMDC_MONGO_HOST\"),\n",
    "    username=\"dwinston_rw\",\n",
    "    password=os.getenv(\"NMDC_MONGO_RW_PWD\")\n",
    ")\n",
    "\n",
    "dbname = \"dwinston_dev\"\n",
    "db = client[dbname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset_database(db)\n",
    "db.list_collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_fetch = [{\n",
    "    \"url\": \"https://portal.nersc.gov/cfs/m3408/meta/img_mg_annotation_objects.json\",\n",
    "    \"type\": \"activity_set\",\n",
    "}, {\n",
    "    \"url\": \"https://portal.nersc.gov/cfs/m3408/meta/img_mg_annotation_data_objects.json\",\n",
    "    \"type\": \"data_object_set\",\n",
    "}, {\n",
    "    \"url\": \"https://portal.nersc.gov/cfs/m3408/meta/mt_annotation_objects.json\",\n",
    "    \"type\": \"activity_set\"\n",
    "}, {\n",
    "    \"url\": \"https://portal.nersc.gov/cfs/m3408/meta/mt_annotation_data_objects.json\",\n",
    "    \"type\": \"data_object_set\"\n",
    "}, {\n",
    "    \"url\": \"https://portal.nersc.gov/cfs/m3408/meta/ReadbasedAnalysis_activity.json\",\n",
    "    \"type\": \"activity_set\"\n",
    "}, {\n",
    "    \"url\": \"https://portal.nersc.gov/cfs/m3408/meta/ReadbasedAnalysis_data_objects.json\",\n",
    "    \"type\": \"data_object_set\"\n",
    "}, {\n",
    "    \"url\": \"https://portal.nersc.gov/cfs/m3408/meta/MAGs_activity.json\",\n",
    "    \"type\": \"mags_activity_set\",\n",
    "}, {\n",
    "    \"url\": \"https://portal.nersc.gov/cfs/m3408/meta/MAGs_data_objects.json\",\n",
    "    \"type\": \"data_object_set\"\n",
    "}, {\n",
    "    \"url\": \"https://nmdcdemo.emsl.pnnl.gov/metabolomics/registration/gcms_metabolomics_data_products.json\",\n",
    "    \"type\": \"data_object_set\"\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for spec in to_fetch:\n",
    "    fetch_conform_and_persist(spec, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "error_urls = fetch_conform_and_persist_from_manifest({\n",
    "    \"url_manifest\": (\"https://nmdcdemo.emsl.pnnl.gov/metabolomics/registration/\"\n",
    "                     \"gcms_metabolomics_metadata_products.json\"),\n",
    "    \"type\": \"activity_set\"\n",
    "}, db)\n",
    "len(error_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MetaG annotations (`/global/project/projectdirs/m3408/www/meta/anno2/*_annotations.json`) are 155 JSON files totalling  ~83GB. To load them into MongoDB, I\n",
    "1. Set up a Globus transfer from NERSC DTN to a Globus Connect Personal endpoint on my laptop. I could e.g.\n",
    "```\n",
    "$ scp dtn01.nersc.gov:/global/project/projectdirs/m3408/www/meta/anno2/*_annotations.json .\n",
    "```\n",
    "but I chose to use Globus, and it works well.\n",
    "2. I have a bash script that uses GNU sed to transform each JSON file to a simple json array, as expected by `mongoimport`:\n",
    "\n",
    "```bash\n",
    "# trim.sh\n",
    "\n",
    "task(){\n",
    "    echo $datafile\n",
    "    gsed -e '1,2d' -e '$d' -e '3i\\[' $datafile > anno2/$(basename $datafile)\n",
    "}\n",
    "\n",
    "for datafile in ~/globus-nersc/nmdc/m3408/www/meta/anno2/*_annotations.json; do\n",
    "    task $datafile &\n",
    "done\n",
    "```\n",
    "I use `ps aux | grep gsed | wc -l` to monitor the progress of the parallel sed tasks. I found that trying to do this head/tail file trimming by `json.load`ing the files in Python and resaving was quite slow because the JSON files are individually quite large.\n",
    "3. I have a bash script that `mongoimport`s each json array file to the database\n",
    "```bash\n",
    "# mongoimport.sh\n",
    "\n",
    "n=$(ls anno2/*_annotations.json | wc -l | xargs) # `| xargs` to trim whitespace\n",
    "i=1\n",
    "for datafile in anno2/*_annotations.json; do\n",
    "    echo \"($i of $n): $datafile\"\n",
    "    mongoimport --uri \"mongodb://<user>:<pwd>@<host>/?authSource=admin\" \\\n",
    "        --jsonArray -d dwinston_share -c raw.functional_annotation_set \\\n",
    "        -j 8 $datafile\n",
    "    i=$((i+1))\n",
    "done\n",
    "```\n",
    "specifying multiple (8 in this case) insertion workers per import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc = time()\n",
    "\n",
    "print(f\"{toc - tic} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmdc",
   "language": "python",
   "name": "nmdc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
